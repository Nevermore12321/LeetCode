# KVM原理

[toc]

## Intel 硬件虚拟化技术

### 1. CPU虚拟化

<u>CPU 是计算机系统最核心的模块</u>，程序执行到最后都是翻译为机器语言在 CPU 上执行的。通常使用**二进制翻译（binary
translation）**来实现虚拟客户机中 CPU 指令的执行，但这样比较复杂，效率比较低。

Intel 在处理器级别提供了对虚拟化技术的支持，被称为 **VMX（virtual-machine extensions）**。

有两种 VMX 操作模式：

- **VMX 根操作（root operation）**：作为虚拟机监控器（VMM）中的 KVM 就是运行在根操作模式下
- **VMX 非根操作（non-root operation）**：虚拟机客户机的整个软件栈（包括操作系统和应用程序）则运行在非根操作模式下

进入 VMX 非根操作模式被称为 “**VM Entry**” ；从非根操作模式退出，被称为 “**VM Exit**”。

注意：**VMX 非根模式的敏感指令会退出到 VMM 中，从而进到 VMX 的根模式执行**。

一个虚拟机监控器软件（VMM）的最基础的运行生命周期及其与客户机的交互如图所示:

![VMM与Guest之间的交互](https://raw.githubusercontent.com/Nevermore12321/LeetCode/blog/%E4%BA%91%E8%AE%A1%E7%AE%97/kvm/VMM%E4%B8%8EGuest%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%A4%E4%BA%92.PNG)

具体的交互过程：

1. 软件通过执行 VMXON 指令进入 VMX 操作模式下；
2. 在 VMX 模式下通过 VMLAUNCH 和 VMRESUME 指令进入客户机执行模式，即 VMX 非根模式；
3. 当在非根模式下触发 VM Exit 时，处理器执行控制权再次回到宿主机的虚拟机监控器上；
4. 最后虚拟机监控可以执行 VMXOFF 指令退出 VMX 执行模式

这里列举部分在非根模式下会导致 “VM Exit” 的敏感指令和一些异常，会导致 VM Exit 并且进入到 VMX 的根模式进行处理的指令：

- <u>一定会导致 VM Exit 的指令</u>：CPUID、GETSEC、INVD、XSETBV 等，以及 VMX 模式引入的 INVEPT、INVVPID、VMCALL、VMCLEAR、VMLAUNCH、VMPTRLD、VMPTRST、VMRESUME、VMXOFF、VMXON 等。
- <u>在一定的设置条件下会导致 VM Exit 的指令</u>：CLTS、HLT、IN、OUT、INVLPG、INVPCID、LGDT、LMSW、MONITOR、MOV from CR3、MOV to CR3、MWAIT、MWAIT、RDMSR、RWMSR、VMREAD、VMWRITE、RDRAND、RDTSC、XSAVES、XRSTORS 等。如在处理器的虚拟机执行控制寄存器中的“HLT exiting”比特位被置为 1 时，HLT 的执行就会导致 VM Exit。
- <u>可能会导致 VM Exit 的事件</u>：一些异常、三次故障（Triple fault）、外部中断、不可屏蔽中断（NMI）、INIT信号、系统管理中断（SMI）等。如在虚拟机执行控制寄存器中的 “NMI exiting” 比特位被置为 1 时，不可屏蔽中断就会导致 VM Exit。

注意：

由于发生一次 VM Exit 的代价是比较高的（可能会消耗成百上千个 CPU 执行周期，而平时很多指令是几个 CPU 执行周期就能完成），所以**对于 VM Exit 的分析是虚拟化中性能分析和调优的一个关键点**。

### 2. 内存虚拟化

**内存虚拟化的目的**<u>是给虚拟客户机操作系统提供一个从0地址开始的连续物理内存空间，同时在多个客户机之间实现隔离和调度</u>

在虚拟化环境中，内存地址的访问会主要涉及以下4个基础概念，如下图所示：

![虚拟化环境下的内存地址](https://raw.githubusercontent.com/Nevermore12321/LeetCode/blog/%E4%BA%91%E8%AE%A1%E7%AE%97/kvm/%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80.PNG)

上图中涉及到了四个地址概念，分别是：

1. **客户机虚拟地址，GVA（Guest Virtual Address）**
2. **客户机物理地址，GPA（Guest Physical Address）**
3. **宿主机虚拟地址，HVA（Host Virtual Address）**
4. **宿主机物理地址，HPA（Host Physical Address）**

**内存虚拟化就是要将客户机虚拟地址（GVA）转化为最终能够访问的宿主机上的物理地址（HPA）**

虚拟机监控器(VMM)就需要维护从客户机虚拟地址到宿主机物理地址之间的一个映射关系，在没有硬件提供的内存虚拟化之前，这个维护映射关系的页表叫作**影子页表（Shadow Page Table）**。Intel CPU 在硬件设计上就引入了 **EPT（Extended Page Tables，扩展页表）**，从而<u>将客户机虚拟地址到宿主机物理地址的转换通过硬件来实现。</u>这个转换过程如下图:

![基于EPT的内存地址转换](https://raw.githubusercontent.com/Nevermore12321/LeetCode/blog/%E4%BA%91%E8%AE%A1%E7%AE%97/kvm/%E5%9F%BA%E4%BA%8EEPT%E7%9A%84%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2.PNG)

过程为：

1. 通过客户机 CR3 寄存器<u>将客户机虚拟地址转化为客户机物理地址</u>

2. 通过查询 EPT 来实现<u>客户机物理地址到宿主机物理地址的转化</u>。

    

EPT 的控制权在虚拟机监控器（VMM）中，只有**当 CPU 工作在非根模式时才参与内存地址的转换**。使用 EPT 后，客户机在读写 CR3 和执行 INVLPG 指令时不会导致 VM Exit，而且客户页表结构自身导致的页故障也不会导致 VM Exit。所以**通过引入硬件上 EPT 的支持，简化了内存虚拟化的实现复杂度，同时也提高了内存地址转换的效率**。

除了 EPT，Intel 在内存虚拟化效率方面还引入了 **VPID（Virtual-processor identifier）特性**，在硬件级对 TLB 资源管理进行了优化。（TLB，Translation Lookaside Buffer 页表缓存/变换旁查缓冲器）。

在没有 VPID 之前，不同客户机的逻辑 CPU 在切换执行时需要刷新 TLB，而 TLB 的刷新会让内存访问的效率下降。<u>VPID 技术通过在硬件上为 TLB 增加一个标志，可以识别不同的虚拟处理器的地址空间，所以系统可以区分虚拟机监控器和不同虚拟机上不同处理器的 TLB，在逻辑 CPU 切换执行时就不会刷新 TLB，而只需要使用对应的 TLB 即可</u>。

### 3. I/O 虚拟化

在虚拟化的架构下，虚拟机监控器必须支持来自客户机的 I/O 请求。通常情况下有以下 4 种 I/O 虚拟化方式：

1. **设备模拟**：在虚拟机监控器中模拟一个传统的 I/O 设备的特性，比如在 QEMU 中模拟一个 Intel 的千兆网卡或者一个 IDE 硬盘驱动器，在客户机中就暴露为对应的硬件设备。<u>客户机中的 I/O 请求都由虚拟机监控器捕获并模拟执行后返回给客户机</u>。
2. **前后端驱动接口**：<u>在虚拟机监控器与客户机之间定义一种全新的适合于虚拟化环境的交互接口</u>，比如常见的 virtio 协议就是在客户机中暴露为 virtio-net、virtio-blk 等网络和磁盘设备，在 QEMU 中实现相应的 virtio 后端驱动。
3. **设备直接分配**：将一个物理设备，如一个网卡或硬盘驱动器<u>直接分配给客户机使用</u>，这种情况下 I/O 请求的链路中很少需要或基本不需要虚拟机监控器的参与，所以<u>性能很好</u>。
4. **设备共享分配**：其实是<u>设备直接分配方式的一个扩展</u>。在这种模式下，<u>一个（具有特定特性的）物理设备可以支持多个虚拟机功能接口，可以将虚拟功能接口独立地分配给不同的客户机使用</u>。如 SR-IOV 就是这种方式的一个标准协议。

常见I/O虚拟化方式的优缺点如下表：

| 方式           | 优点                                 | 缺点                                                         |
| -------------- | ------------------------------------ | ------------------------------------------------------------ |
| 设备模拟       | 兼容性好                             | 1. 性能较差<br />2. 模拟设备的功能特性支持不够多             |
| 前后端驱动接口 | 性能所有提升                         | 1. 兼容性差一些：因为以来客户机中安装依赖<br />2. I/O 压力大时，后端驱动的 CPU 占用率高 |
| 设备直接分配   | 性能非常好                           | 1. 需要硬件设备的特性支持<br />2. 单个设备只能分配一个客户机<br />3. 很难支持动态迁移 |
| 设备共享分配   | 1. 性能非常好<br />2. 单个设备可共享 | 1. 需要硬件设备的特性支持<br />2. 很难支持动态迁移           |

**设备直接分配在 Intel 平台上就是 VT-d（Virtualization Technology For Directed I/O）特性**，一般在系统BIOS中可以看到相关的参数设置。

- Intel VT-d 为虚拟机监控器提供了几个重要的能力：I/O 设备分配、DMA 重定向、中断重定向、中断投递等。
- 缺点：**单个设备只能分配给一个客户机**，而在虚拟化环境下一个宿主机上往往运行着多个客户机，很难保证每个客户机都能得到一个直接分配的设备。

为了克服这个缺点，**设备共享分配硬件技术**就应运而生，其中 **SR-IOV（Single Root I/OVirtualization and Sharing）**就是这样的一个标准。实现了 SR-IOV 规范的设备，有一个功能完整的 PCI-e 设备成为物理功能（Physical Function，PF）。

- 在使能了 SR-IOV 之后，PF 就会派生出若干个虚拟功能（Virtual Function，VF）。VF看起来依然是一个 PCI-e 设备，它拥有最小化的资源配置，有用独立的资源，可以作为独立的设备直接分配给客户机使用。
- 例如，Intel 的很多高级网卡如 82599 系列网卡就支持 SR-IOV 特性，一个85299 网卡 PF 就即可配置出多达 63 个 VF，基本可满足单个宿主机上的客户机分配使用。

### Intel虚拟化技术发展

Intel硬件虚拟化技术大致分为如下 3 个类别（这个顺序也基本上是相应技术出现的时间先后顺序）：

1. **VT-x 技术**：是指 Intel 处理器中进行的一些虚拟化技术支持，包括 CPU 中引入的最基础的 VMX 技术，使得 KVM 等硬件虚拟化基础的出现成为可能。同时也包括内存虚拟化的硬件支持 EPT、VPID 等技术。
2. **VT-d 技术**：是指 Intel 的芯片组的虚拟化技术支持，通过 Intel IOMMU 可以实现对<u>设备直接分配</u>的支持。
3. **VT-c 技术**：是指 Intel 的 <u>I/O 设备相关的虚拟化技术支持</u>，主要包含两个技术：一个是借助虚拟机设备队列（VMDq）最大限度提高 I/O 吞吐率，VMDq 由 Intel 网卡中的专用硬件来完成；另一个是借助虚拟机直接互连（VMDc）大幅提升虚拟化性能，VMDc 主要就是基于 SR-IOV 标准将单个 Intel 网卡产生多个 VF 设备，用来直接分配给客户机。



## KVM架构

KVM就是在硬件辅助虚拟化技术之上构建起来的虚拟机监控器。

并非要所有这些硬件虚拟化都支持才能运行 KVM 虚拟化，**KVM 对硬件最低的依赖是 CPU 的硬件虚拟化支持**，比如：Intel 的 VT 技术和 AMD 的 AMD-V 技术，而其他的内存和 I/O 的硬件虚拟化支持，会让整个 KVM 虚拟化下的性能得到更多的提升。

<u>KVM 是在硬件虚拟化支持下的完全虚拟化技术，所以它能支持在相应硬件上能运行的几乎所有的操作系统。</u>

**KVM 虚拟化的核心主要由以下两个模块组成**：

1. **KVM 内核模块**，<u>它属于标准 Linux 内核的一部分</u>，是一个<u>专门提供虚拟化功能的模块</u>，主要负责 CPU 和内存的虚拟化，包括：客户机的创建、虚拟内存的分配、CPU执行模式的切换、vCPU寄存器的访问、vCPU 的执行。
2. **QEMU 用户态工具**，它是一个普通的 <u>Linux 进程</u>，<u>为客户机提供设备模拟的功能</u>，包括模拟 BIOS、PCI/PCIE 总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。同时它通过 ioctl 系统调用与内核态的 KVM 模块进行交互。

KVM 架构图如下所示：

![KVM虚拟化基础架构](https://raw.githubusercontent.com/Nevermore12321/LeetCode/blog/%E4%BA%91%E8%AE%A1%E7%AE%97/kvm/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.PNG)

注意：

- **每个客户机就是一个 QEMU 进程**，在一个宿主机上有多少个虚拟机就会有多少个 QEMU 进程；
- **客户机中的每一个虚拟 CPU 对应 QEMU 进程中的一个执行线程**；
- **一个宿主机中只有一个 KVM 内核模块，所有客户机都与这个内核模块进行交互**。

### 1. KVM 内核模块

KVM 内核模块是标准 Linux 内核的一部分，**由于 KVM 的存在让 Linux 本身就变成了一个 Hypervisor，可以原生地支持虚拟化功能**。目前，KVM 支持多种处理器平台，它支持最常见的以 Intel 和 AMD 为代表的 x86 和 x86_64 平台，也支持 PowerPC、S/390、ARM 等非x86 架构的平台。

KVM 模块是 KVM 虚拟化的**核心模块**，它在内核中由两部分组成：

- **一个是处理器架构无关的部分**，用 lsmod 命令中可以看到，叫作 kvm 模块；
- **一个是处理器架构相关的部分**，在 Intel 平台上就是 kvm_intel 这个内核模块。

**KVM 的启动过程**：

1. 在被内核加载的时候，KVM 模块会先初始化内部的数据结构；
2. 做好准备之后，KVM 模块检测系统当前的 CPU，然后打开 CPU 控制寄存器 CR4 中的虚拟化模式开关，并通过执行 VMXON 指令将宿主操作系统（包括KVM 模块本身）置于 CPU 执行模式的虚拟化模式中的**根模式**；
3. KVM模块创建特殊**设备文件 /dev/kvm** 并等待来自用户空间的命令。
4. 虚拟机的创建和运行将是一个用户空间的应用程序（QEMU）和 KVM 模块相互配合的过程
5. **内存虚拟化也是由 KVM 模块实现的**，包括前面提到的使用硬件提供的 EPT 特性，通过两级转换实现客户机虚拟地址到宿主机物理地址之间的转换。

注意：

- **/dev/kvm** 这个设备可以被当作一个**标准的字符设备**，**KVM 模块与用户空间 QEMU 的通信接口主要是一系列针对这个特殊设备文件的 ioctl 调用**

**创建虚拟机时 kvm 原理：**

1. "创建虚拟机"可以理解成 KVM 为了某个特定的虚拟客户机（用户空间程序创建并初始化）创建对应的内核数据结构。
2. KVM 还会返回一个文件句柄来代表所创建的虚拟机。针对该文件句柄的 ioctl 调用可以对虚拟机做相应的管理，比如创建用户空间虚拟地址和客户机物理地址及真实内存物理地址的映射关系，再比如创建多个可供运行的虚拟处理器（vCPU）。
3. KVM 模块会为每一个创建出来的虚拟处理器生成对应的文件句柄，对虚拟处理器相应的文件句柄进行相应的 ioctl 调用，就可以对虚拟处理器进行管理。

### 2. QEMU用户态设备模拟

QEMU 最初实现的虚拟机是一个**纯软件**的实现，通过**二进制翻译**来实现虚拟化客户机中的 CPU 指令模拟，所以<u>性能比较低</u>。但是，其<u>优点是跨平台</u>，QEMU 支持在 Linux、Windows、FreeBSD、Solaris、MacOS 等多种操作系统上运行，能支持在 QEMU 本身编译运行的平台上就实现虚拟机的功能，甚至可以支持客户机与宿主机并不是同一个架构（比如在 x86 平台上运行 ARM 客户机）。

**每一个虚拟客户机在宿主机中就体现为一个 QEMU 进程，而客户机的每一个虚拟 CPU 就是一个 QEMU 线程**。

- 虚拟机运行期间，QEMU 会通过 KVM 模块提供的系统调用进入内核，由 KVM 模块负责将虚拟机置于处理器的特殊模式下运行。
- 遇到虚拟机进行 I/O 操作时，KVM 模块会从上次的系统调用出口处返回 QEMU，由 QEMU 来负责解析和模拟这些设备。

**QEMU 使用了 KVM 模块的虚拟化功能，为自己的虚拟机提供硬件虚拟化的加速，从而极大地提高了虚拟机的性能**。除此之外，虚拟机的配置和创建，虚拟机运行依赖的虚拟设备，虚拟机运行时的用户操作环境和交互，以及一些针对虚拟机的特殊技术（如：动态迁移），都是由 QEMU 自己实现的。

**QEMU 既是一个功能完整的虚拟机监控器，也在 QEMU/KVM 的软件栈中承担设备模拟的工作。QEMU是一个VMM，也就是 Hypervisor**



### 与QEMU/KVM结合的组件

1. **vhost-net**
    - vhost-net 是 Linux内核中的一个模块
    - 用于替代 QEMU 中的 virtio-net 用户态的 virtio 网络的后端实现。
    - 使用 vhost-net 时，还<u>支持网卡的多队列，整体来说会让网络性能得到较大提高</u>。
2. **Open vSwitch**
    - Open vSwitch 是一个高质量的、多层虚拟交换机
    - 目的是让大规模网络自动化可以通过编程扩展，同时仍然支持标准的管理接口和协议（例如 NetFlow、sFlow、SPAN、RSPAN、CLI、LACP、
        802.1ag）。
    - 也提供了对 OpenFlow 协议的支持，用户可以使用任何支持 OpenFlow 协议的控制器对OVS进行远程管理控制。
    - Open vSwitch 被设计为支持跨越多个物理服务器的分布式环境
    - Open vSwitch 支持多种虚拟化技术，包括 Xen/XenServer、KVM 和 VirtualBox。
    - 在KVM虚拟化中，要实
        现软件定义网络（SDN），那么Open vSwitch是一个非常好的开源选择。
3. **DPDK**
    - DPDK全称是 Data Plane Development Kit，为 Intel x86 处理器架构下<u>用户空间高效的数据包处理提供库函数和驱动的支持</u>
    - 支持POWER和ARM处理器架构。
    - 专注于网络应用中数据包的高性能处理。
    - <u>具体体现在 DPDK 应用程序是运行在用户空间上，利用自身提供的数据平面库来收发数据包，绕过了 Linux 内核协议栈对数据包处理过程。其优点是：性能高、用户态开发、出故障后易恢复</u>。
    - 在 KVM 架构中，为了达到非常高的网络处理能力（特别是小包处理能力），可以选择 DPDK 与QEMU 中的 vhost-user 结合起来使用。
4. **SPDK**
    - SPDK全称是 Storage Performance Development Kit，可为编写高性能、可扩展的、用户模式的存储程序提供一系列工具及开发库。
    - 与 DPDK 非常类似，其主要特点是：将驱动放到用户态从而实现零拷贝、用轮询模式替代传统的中断模式、在所有的 I/O 链路上实现无锁设计，这些设计会使其性能比较高。
    - 在 KVM 中需要非常高的存储 I/O 性能时，可以将 QEMU 与 SPDK 结合使用。
5. **Ceph**
    - Ceph 是 Linux 上一个著名的<u>分布式存储系统</u>，能够在维护 POSIX 兼容性的同时加入复制和容错功能。
    - Ceph 由储<u>存管理器</u>（Object storage cluster对象存储集群，即OSD守护进程）、<u>集群监视器</u>（Ceph Monitor）和<u>元数据服务器</u>（Metadata server cluster，MDS）构成。
    - Ceph 支持 3 种调用接口：<u>对象存储，块存储，文件系统挂载</u>。
    - 在 libvirt 和 QEMU 中都有 Ceph 的接口，所以 Ceph 与 KVM 虚拟化集成是非常容易的。
6. **libguestfs**
    - libguestfs 是<u>用于访问和修改虚拟机的磁盘镜像的一组工具集合</u>。
    - libguestfs 提供了访问和编辑客户机中的文件、脚本化修改客户机中的信息、监控磁盘使用和空闲的统计信息、P2V、V2V、创建客户机、克隆客户机、备份磁盘内容、格式化磁盘、调整磁盘大小等非常丰富的功能。
    - libguestfs 还提供了共享库，可以在 C/C++、Python 等编程语言中对其进行调用。
    - libguestfs 不需要启动 KVM 客户机就可以对磁盘镜像进行管理，功能强大且非常灵活，是管理 KVM 磁盘镜像的首选工具。

### KVM上层管理工具

1. **libvirt**
    - **libvirt 是使用最广泛的对 KVM 虚拟化进行管理的工具和应用程序接口，已经是事实上的虚拟化接口标准**
    - 后面介绍的其他工具都是基于 libvirt 的 API 来实现的。
    - 作为通用的虚拟化 API，libvirt 不但能管理 KVM，还能管理 VMware、Hyper-V、Xen、VirtualBox 等其他虚拟化方案。
2. **virsh**
    - **virsh 是一个常用的管理 KVM 虚拟化的命令行工具**
    - 对于系统管理员在单个宿主机上进行运维操作，virsh 命令行可能是最佳选择。
    - virsh是用C语言编写的一个使用 libvirt API 的虚拟化管理工具，其源代码也是在 libvirt 这个开源项目中的。
3. **virt-manager**
    - **virt-manager 是专门针对虚拟机的图形化管理软件**
    - 底层与虚拟化交互的部分仍然是调用libvirt API来操作的。
    - virt-manager 除了提供虚拟机生命周期（包括：创建、启动、停止、打快照、动态迁移等）管理的基本功能，还提供性能和资源使用率的监控，同时内置了 VNC 和 SPICE 客户端，方便图形化连接到虚拟客户机中。
    - 在管理的机器数量规模较小时，virt-manager 是很好的选择。
4. **OpenStack**
    - **OpenStack 是一个开源的基础架构即服务（IaaS）云计算管理平台，可用于构建共有云和私有云服务的基础设施。**
    - OpenStack 是目前业界使用最广泛的功能最强大的云管理平台，它不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如：对象存储、块存储、网络、镜像、身份验证、编排服务、控制面板等。
    - OpenStack 仍然使用 libvirtAPI 来完成对底层虚拟化的管理。

