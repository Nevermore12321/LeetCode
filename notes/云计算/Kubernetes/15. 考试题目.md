[toc]


# 镜像使用

只能登录console这台虚拟机，其他kubernetes集群的root密码均未知  
登录用户名：tom  
密码：redhat  


# 考试题目

## 切换集群环境

最好在做每个题目时，都设置一下集群环境：
```
kubectl config use-context k8s
```


## 配置kubectl自动补全

1. 查看kubectl补全的命令
```
kubectl --help | grep bash
```
2. 将下面的命令添加到/etc/profile中
```
source < (kubectl completion bash)
```
3. 加载 /etc/profile 文件
```
source /etc/profile
```




## k8s 集群题目

### 1. ClusterRole, ServiceAccount, ClusterRoleBinding

#### 题目

设置配置环境 kubectl config use-context k8s

**Context**  
为部署管道创建一个新的 ClusterRole 并将其绑定到范围为特定 namespace 的特定 ServiceAccount 

**Task**  
创建一个名字为 deployment-clusterrole 且仅允许创建以下资源类型的新 ClusterRole ：
- Deployment
- StatefulSet
- DaemonSet

在现有的 namespace app-team1 中创建有个名为 cicd-token 的新 ServiceAccount 。

限于 namespace app-team1 ，将新的 ClusterRole deployment-clusterrole 绑定到新的 ServiceAccount cicd-token。


#### 解答
1. 切换集群：
```
kubectl config use-context k8s
```
2. 创建 clusterrole, 名称为 deployment-clusterrole
    - 查看帮助：`kubectl create clusterrole --help | head`
    - 创建clusterrole，并分配权限：`kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulsets,daemonsets`
    - 注意：--verb 表示 权限，题目是创建 create
    - 注意：--resource 表示哪些类型，将题目中的类型变为小写，并且最后加s
3. 创建 serviceaccount，名称为 cicd-token, 命名空间为 app-team1
    - 命令：`kubectl create sa cicd-token -n app-team1`
    - 注意：指定明确的命名空间 app-team1
4. 将 sa 与 clusterrole 绑定，创建 clusterrolebinding
    - 查看帮助：`kubectl create clusterrolebinding --help | head`
    - 命令: `kubectl create clusterrolebinding aaa --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token`
    - 注意：由于题目没有指定 clusterrolebinding 的名称，可以自定义
    - 注意：--clusterrole 指定绑定到哪个 clusterrole，
    - 注意：--serviceaccount 指定绑定哪个 sa，注意格式：namespace:sa名称



### 2. NetworkPolicy

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

创建一个名为 allow-port-from-namespace 的新 NetworkPolicy，以允许现有 namespace my-app
中的 Pods 连接到同一 namespace 中其他 pods 的端口 9200 。

确保新的 NetworkPolicy：
- 不允许对没有在监听端口 9200 的 pods 访问
- 不允许不来自 namespace my-app 的 pods 的访问

#### 解答

参考网站：https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/
1. 设置集群环境：
```
kubectl config use-context k8s
```
2. 如果 命名空间 my-app 不存在，则创建 该 命名空间
```
kubectl create ns my-app
```
3. 给 该 namespace my-app 添加一个标签，用以后面 networkpolicy 选择
    - 先查看 my-app 命名空间的标签，如果有特殊的标签，则不需要添加标签，如果是新建的命名空间则需要添加唯一的标签：`kubectl get ns --show-labels`
    - 给 my-app 命名空间添加唯一的标签：`kubectl label ns my-app name=my-app`
4. 创建 networkpolicy 
    - 创建 yaml 文件, 名称可以随意：
    ```
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: allow-port-from-namespace
      namespace: my-app
    spec:
      podSelector:
        matchLabels:
      policyTypes:
      - Ingress
      ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              name: my-app
        ports:
        - protocol: TCP
          port: 9200
    ```
    - 注意：空的 podSelector 选择命名空间下的所有 Pod。
    - 注意：只限制 进来的 流量
    - 创建 networkpolicy：`kubectl apply -f networkpolicy.yaml`



### 3. Service

#### 题目

设置配置环境 kubectl config use-context k8s

**Task** 

请重新配置现有的部署 front-end 以及添加名为 http 的端口规范来公开现有容器 nginx 的端口 80/tcp 。


创建一个名为 front-end-svc 的新服务，以公开容器端口 http 。
配置此服务，以通过在排定的节点上的 NodePort 来公开各个 pods。


#### 解答

1. 设置配置环境 
```
kubectl config use-context k8s
```
2. 为 名为 front-end 的 deployment 添加 名为 http 的端口
    - 通过 edit 来编辑该 deployment : `kubectl edit deployment front-end`
    - 在 spec.template.spec.containers 添加 ports信息：
    ```
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
    ```
    - 保存退出
3. 创建  front-end-svc 服务，使用 NodePort 类型，将http端口暴露出去
    - 命令：`kubectl expose deployment front-end --name=front-end-svc --port=80 --type=NodePort`



### 4. ingress

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

如下创建一个新的 nginx ingress 资源：  
- 名称： pong
- namespace： ing-internal
- 使用服务端口 5678 在路径 /hello 上公开服务 hello

> 可以使用一下命令检查服务 hello 的可用性，该命令返回 hello：
> curl -kL < INTERNAL_IP>/hello/


#### 解答：

参考网址：https://kubernetes.io/zh/docs/concepts/services-networking/ingress/

1. 设置集群：
```
kubectl config use-context k8s
```
2. 创建 ingress.yaml 文件，内容为：
    - 注意：后端的服务名称为 hello
    - 注意：联系环境 annotations 下的 那一行需要注释掉
    - 注意：考试不需要
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pong
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              number: 5678
```
3. 创建 ingress，并验证
    - 命令: `kubectl apply -f ingress.yaml`
    - 验证：`curl -kL < INTERNAL_IP>/hello/`



### 5. 扩展 scale

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

将 deployment 从 webserver 扩展至 6pods

#### 解答

1. 设置集群：
```
kubectl config use-context k8s
```
2. 扩展 pod 数量
    - 命令：`kubectl scale deployment webserver --replicas=6`
3. 验证
    - 命令：`kubectl get deployment`


### 6. pod 的调度

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  
按如下要求调度一个 pod：
- 名称： nginx-kusc00401
- image: nginx
- Node selector: disk=ssd


#### 解答

参考网站：https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-pods-nodes/

1. 设置集群：
```
kubectl config use-context k8s
```
2. 创建pods.yaml文件，内容为：
    - 也可以用：`kubectl run nginx-kusc00401 --image=nginx --dry-run=client -o yaml > pods.yaml
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeSelector:
    disk: ssd
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
```
3. 创建 pod
    - 命令: `kubectl apply -f pods.yaml`




### 7. 污点  taint


#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

检查有多少个 worker nodes 已准备就绪（不包括被打上 Taint: NoSchedule 的节点），并将数量写入/opt/KUSC00402/kusc00402.txt


#### 解答
1. 设置集群
```
kubectl config use-context k8s
```
2. 查看集群所有节点的名称
    - 命令：`kubectl get nodes`
3. 注意：除去master的工作节点，查看工作节点，是否为ready，并且是否存在污点taint
    - 查看是否ready：`kubectl get nodes`
    - 查看所有工作节点的污点情况，`kubectl describ nodes vms22.rche.cc | grep Taints`
    - 如果没有污点，则表示工作节点正常，计算所有的正常节点数量，写入文件/opt/KUSC00402/kusc00402.txt ： `echo 2 > /opt/KUSC00402/kusc00402.txt`



### 8. 多 container

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

创建一个名字为 kucc4 的pod，在pod 里面分别为以下每个images单独运行一个app container
（可能会有 1-4 个 images）：
nginx+redis+memcached+consul

#### 解答

1. 设置集群
```
kubectl config use-context k8s
```
2. 创建 pod 模板yaml文件
    - 命令：`kubectl run kucc4 --image=nginx --dry-run=client -o yaml > 11-pod.yaml`
3. 修改配置文件为：
```
apiVersion: v1
kind: Pod
metadata:
  name: kucc4
  labels:
    run: kucc4
spec:
  containers:
  - name: c1
    image: nginx
    imagePullPolicy: IfNotPresent
  - name: c2
    image: redis
    imagePullPolicy: IfNotPresent
  - name: c3
    image: memcached
    imagePullPolicy: IfNotPresent
  - name: c4
    image: consul
    imagePullPolicy: IfNotPresent
```
4. 创建 pod
    - 命令：`kubectl apply -f 11-pod.yaml`



### 9. persistentvolume PV

#### 题目

设置配置环境 kubectl config use-context k8s 

**Task**

创建名为 app-data 的 persistent volume，容量为 1Gi ，访问模式为 ReadWriteMany 。volume 类型为 hostPath ，位于 /srv/app-data


#### 解答

参考网站：https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/

1. 设置集群
```
kubectl config use-context k8s
```
2. 拷贝 pv 的模板为 12-pv.yaml , 并修改，内容为：
    - 注意：storageClassName 没有要求，因此可以删除
    - 注意：修改 type 为 hostpath
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-data
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /srv/app-data
```


### 10. pvc 以及 pod 使用 pvc

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

创建一个新的 PersistentVolumeClaim：
- 名称： pvvolume
- class： csi-hostpath-sc
- 容量： 10Mi


创建一个新的 pod，此 pod 将作为 volume 挂载到 PersistentVolumeClaim：
- 名称： web-server
- image: nginx
- 挂载路径: /usr/share/nginx/html
- 配置新的 pod，以对 volume 具有 ReadWriteOnce 权限。


最后，使用 kubectl edit 或者 kubectl patch 将 PersistentVolumeClaim 的容量扩展为 70Mi ，并记录此次更改。



#### 解答

参考网站：https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/

1. 设置集群
```
kubectl config use-context k8s
```
2. 创建 pvc 的 yaml 文件 13-pvc.yaml，通过官方网站查找模板，内容为：
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvvolume
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Mi
  storageClassName: csi-hostpath-sc
```
3. 创建 pvc
    - 命令：`kubectl apply -f 13-pvc.yaml`
4. 创建 pod  的 yaml 文件 13-pod.yaml, 通过官网查找模板，内容为：
```
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/usr/share/nginx/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: pvvolume
```
5. 创建 pod
    - 命令：`kubectl apply -f 13-pod.yaml`
6. 扩展 pvc 为 70Mi
    - 通过 `kubectl edit pvc pvvolume` 修改 pvc 的容量
    - 修改的地方有两处：
        - `resources.requests.storage` 改成 70Mi
        - `status.capacity.storage` 改成 70Mi



### 11. logs 日志

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

监控 pod foo 的日志并：
- 提取与错误 unable-to-access-website 相对应的日志行
- 将这些日志行写入到 /opt/KUTR00101/foo


#### 解答


1. 设置集群
```
kubectl config use-context k8s
```
2. 使用 logs 命令，并通过 grep 过滤，写入到文件
    - 命令：`kubectl logs foo | grep unable-to-access-website > /opt/KUTR00101/foo`





### 12. volume 挂载

#### 题目

设置配置环境 kubectl config use-context k8s

**Context**  

在不更改其现有容器的情况下，需要将一个现有的 pod 集成到 kubernetes 的内置日志记录
体系结构中（例如 kubectl logs）。添加 streamimg sidecar 容器是实现此要求的一种好方法。

**Task**  

将一个 busybox sidecar 容器添加到现有的 pod legacy-app 。新的 sidecar 容器必须运行一下命令：  
`/bin/sh -c tail -n+1 /var/log/legacy-app.log`  


使用名为 logs 的 volume mount 来让文件 /var/log/legacy-app.log 可用于 sidecar 容器。

> 不要更改现有容器。不要修改日志文件的路径，两个容器必须通过/var/log/legacy-app.log 来访问该文件。


#### 解答

1. 设置集群
```
kubectl config use-context k8s
```
2. 将 legacy-app 这个pod，产生一个 yaml 文件模板，在这个模板上修改
    - 命令：`kubectl get pod legacy-app -o yaml > 15-pod.yaml`
    - 最好先备份一下：`cp 15-pod-bakup.yaml`
3. 修改 yaml 文件的三个地方：
    - 在 volumes 中定义一个卷，让两个容器共同挂载到同一个地方：
    ```
    volumes:
    - name: logs
      emptyDir: {}
    ```
    - 在 containers 中添加一个 容器，sidecar，并且让这个容器挂载 logs 卷 到 /var/log 目录
    ```
    containers:
    - name: c1
      image: busybox
      command: ["/bin/sh","-c","tail -n+1 /var/log/legacy-app.log"]
      volumeMounts:
      - name: logs
        mountPath: /var/log
    ```
    - 在 主容器 中，添加 volumeMounts ，同样 将 logs 卷 挂载到 /var/log 目录
    ```
    volumeMounts:
      - name: logs
        mountPath: /var/log
    ```
4. 删除原先的 pod 
    - 命令： `kubectl delete pod legacy-app`
5. 创建 新的 pod
    - 命令：`kubectl apply -f 15-pod.yaml`
6. 总体思路就是，添加一个容器 sidecar 使得 该容器 与 主容器同时挂载一个卷




### 13.  top 查看资源使用情况

#### 题目

设置配置环境 kubectl config use-context k8s

**Task**  

通过 pod label name=cpu-user ，找到运行时占用大量 CPU 的 pod，

并将占用 CPU 最高的 pod 名称写入到文件 /opt/KUTR000401/KUTR00401.txt （已存在）

#### 解答

1. 切换集群 
```
kubectl config use-context ek8s
```
2. 使用 top 来查看指定 label 的pods 的资源使用情况，
    - 命令：`kubectl top pods -l name=cpu-user`
3. 找到 CPU 最高的 pod 名称，写入到文件中
    - 注意：练习环境中的资源使用是相同的，考试时不同的
    - 命令：`echo 名称 > /opt/KUTR000401/KUTR00401.txt`
4. 如果目录存在，直接把结果写入到 /opt/KUTR000401/KUTR00401.txt 文件中
5. 如果目录不存在，考试中的目录为 /opt/KUTR00401/KUTR00401.txt（目录少一个0），这时
    - 先把内容写到  考试机器有的目录，也就是少一个0的目录文件中 /opt/KUTR00401/KUTR00401.txt
    - 然后，按照题目要求，
        - 使用root 用户，创建题目中的目录， 也就是3个0的目录 : `sudo mkdir /opt/KUTR00401/`
        - 然后创建文件：`sudo touch /opt/KUTR000401/KUTR00401.txt`
        - 然后将结果写入到这个目录的文件中，
        - 然后修改权限: `chown -R student.student /opt/KUTR000401/`





## ek8s 集群题目

### 14. drain 驱逐node

#### 题目

设置配置环境 kubectl config use-context ek8s

**Task**  

将名为 ek8s-node-0 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。


#### 解答

1. 切换集群 
```
kubectl config use-context ek8s
```
2. 查看集群所有节点:
```
kubectl get nodes
```
3. 驱逐名称为 ek8s-node-0 的节点
    - 命令：`kubectl drain ek8s-node-0 --ignore-daemonsets --force`
    - 注意：--ignore-daemonsets 必须加，必须忽略 daemonsets 的驱逐
    - 注意：--force 可以加，表示强制驱逐
    - 在练习中，将 vms25.rhce.cc 这个节点驱逐
4. 查看节点是否有 SchedulingDisable
```
kubectl get nodes
```


### 15. 排除 not ready 故障

#### 题目

设置配置环境 kubectl config use-context ek8s

**Task**

名为 wk8s-node-0 的 kubernetes worker node 处于 Not Ready 状态。调查发生这种情况的原因，并采取相应措施将 node 恢复为 Ready 状态，确保所做的任何更改永久生效。


> 可使用以下命令通过 ssh 连接到故障 node：ssh wk8s-node-0
> 可使用一下命令在该 node 上获取更高权限：sudo -i


#### 解答

1. 切换集群 
```
kubectl config use-context ek8s
```
2. 查看故障节点的名称：
    - 命令： `kubectl get nodes`
    - 看到，wk8s-node-0 节点 not ready
3. ssh 登录到该节点
    - 命令 : `ssh wk8s-node-0`
4. 切换到root权限
    - 命令： `sudo -i`
5. 查看 kubelet 状态：
    - 命令： `systemctl is-active kubelet`
    - 状态为 unknown
6. 启动 kubelet，并设置为开机自启
    - 命令：`systemctl start kubelet`
    - 命令: `systemctl enable kubelet`
7. 切换回 console 节点，再次查看是否变为 ready



## mk8s 集群题目

### 16. 集群升级


#### 题目

设置配置环境 kubectl config use-context mk8s

**Task**   

现有的 kubernetes 集群正在运行的版本是 1.18.8。仅将主节点上的所有 kubernetes 控制平面和节点组件升级到版本 1.19.2 。

另外，在主节点上升级 kubelet 和 kubectl 。

注意：确保在升级前 drain 主节点，并在升级后 uncordon 主节点。请不要升级工作节点，etcd，container 管理器，CNI 插件，DNS 服务或任何其他插件。


#### 解答

参考页面：https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

1. 切换集群 
```
kubectl config use-context mk8s
```
2. 查看集群所有节点:
```
kubectl get nodes
```
3. 注意，需要ssh到要升级的节点上做升级
4. 升级 master 节点
    - ssh 连接：`ssh vms28.rhce.cc`
    - 切换到root权限：`sudo -i`
    - 由于系统是 ubuntu 系统，因此使用 apt 命令升级，找到最新的稳定版 1.19.2
    ```
    apt update
    apt-cache policy kubeadm
    ```
    - 升级 kubeadm :
    ```
    apt-mark unhold kubeadm && \
    apt-get update && apt-get install -y kubeadm=1.19.2-00 && \
    apt-mark hold kubeadm
    ```
    - 验证下载操作正常，并且 kubeadm 版本正确：
    ```
    kubeadm version
    ```
    - 腾空控制平面节点：
    ```
    kubectl drain vms28.rhce.cc --ignore-daemonsets
    ```
    - 查看可升级的版本：
    ```
    kubeadm upgrade plan
    ```
    - 执行升级,注意写对版本：
    ```
    kubeadm upgrade apply v1.19.2
    ```
    - 升级成功后，取消控制节点的维护模式
    ```
    kubectl uncordon vms28.rhce.cc
    ```
    - 在master上升级kubelet、kubectl
    ```
    apt-mark unhold kubelet kubectl && \
    apt-get update && apt-get install -y kubelet=1.19.2-00 kubectl=1.19.2-00 && \
    apt-mark hold kubelet kubectl
    ```
    - 重启 kubelet
    ```
    systemctl daemon-reload
    systemctl restart kubelet
    ```
    - 再次查看 node 版本
    ```
    kubectl get nodes
    ```
    - 退出 master 节点 到 console 节点上
5. 升级 node 节点
    - ssh 连接：`ssh vms29.rhce.cc`
    - 切换到root权限：`sudo -i`
    - 在工作节点升级 kubeadm，注意版本：
    ```
    apt-mark unhold kubeadm && \
    apt-get update && apt-get install -y kubeadm=1.19.2-00 && \
    apt-mark hold kubeadm
    ```
    - 登出 到 console 节点上，将 该node节点驱逐
    ```
    kubectl drain vms29.rhce.cc --ignore-daemonsets
    ```
    - 再次 登录到 node节点上, 并切换到root用户
    ```
    ssh vms29.rhce.cc
    sudo -i
    ```
    - 升级 node 节点：
    ```
    kubeadm upgrade node
    ```
    - 由于题目中没有要求升级 node 节点的 kubelet 和 kubectl，因此不需要升级
    - 退出 vms29.rhce.cc node节点，到 console 节点，将 node 节点取消维护模式
    ```
    kubectl uncordon vms29.rhce.cc
    ```

## etcd 类题目

### 17. etcd 备份

#### 题目

此项目无需更改配置环境

**Task**  

首先为运行在 https://127.0.0.1:2379 上的现有 etcd 实例创建快照并将快照保存到 /srv/data/etcd-snapshot.db 。

> 注意：为给定实例创建快照预计能在几秒钟内完成。如果该操作似乎挂起，则命令可能有问题。用 ctrl+c 来取消操作，然后重试。

然后还原位于 /srv/data/etcd-snapshot-previous.db 的现有先前快照。


> 提供了以下 TLS 证书和密钥，以通过 etcdctl 连接到服务器。
> - CA 证书：/opt/KUIN00601/ca.crt
> - 客户端证书: /opt/KUIN00601/etcd-client.crt
> - 客户端密钥:/opt/KUIN00601/etcd-client.key


#### 解答
1. 查看 etcdctl 命令的版本：
    - 默认是 etcdctl 的 3 版本
```
etcdctl --help 
```
2. etcd 备份
    - 命令： `etcdctl snapshot save --endpoints=https://127.0.0.1:2379 --cacert="/opt/KUIN00601/ca.crt" --cert=" /opt/KUIN00601/etcd-client.crt" --key="/opt/KUIN00601/etcd-client.key" /srv/data/etcd-snapshot-previous.db`
    - 在练习时，直接使用 http://127.0.0.1:2379 , 并且不需要证书，命令为：`etcdctl snapshot save /srv/data/etcd-snapshot-previous.db`
    - 注意，在考试时，写对密钥的路径
3. 还原 etcd 快照
    - 切换到root权限：`sudo -i`
    - 关闭 etcd 服务: `systemctl stop etcd`
    - 查看 etcd 数据存放的目录：`grep DIR /etc/etcd/etcd.conf`
    - 删除 etcd 数据存放的目录中的 default.etcd 目录：`rm -rf /var/lib/etcd/default.etcd`
    - 还原快照：`etcdctl snapshot restore  --cacert="/opt/KUIN00601/ca.crt" --cert=" /opt/KUIN00601/etcd-client.crt" --key="/opt/KUIN00601/etcd-client.key  /srv/data/etcd-snapshot-previous.db `
    - 注意：endpoints 以及 证书还是需要写，但在练习中，可以不写 证书
    - 注意：--data-dir 表示 在 etcd.conf 中的 路径，也就是 /var/lib/etcd/default.etcd
    - 修改目录的属组：`chown -R etcd.etcd /var/lib/etcd/default.etcd/`
    - 重新启动 etcd 服务：`systemctl start etcd`


​    