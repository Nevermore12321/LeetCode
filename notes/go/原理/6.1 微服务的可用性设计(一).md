[toc]


# 隔离

隔离，本质上是对系统或资源进行分割，从而实现当系统发生故障时能限定传播范围和影响范围，即**发生故障后只有出问题的服务不可用，保证其他服务仍然可用。**

隔离可以分为：
- 服务隔离
    - 动静分离
    - 读写分离
- 轻重隔离
    - 核心、快慢、热点
- 物理隔离
    - 线程、进程、集群、机房





## 服务隔离


### 1. 读写分离

实践：主从（master-salve）、Replicaset（多备份）、CQRS（读写分离）


CQRS：Command Query Responsibility Segregation，故名思义是将 command 与 query 分离的一种模式。

直面意思也就是查询一套系统，写数据一套系统


CQRS难点：
- 事务问题：写系统完成一次写后，需要通知读系统，来更新缓存，这中间就需要用到了网络和消息中间件
- 没有成熟的框架，需要自己实现。



### 2. 动静分离


- 小到 CPU 的 cacheline false sharing、数据库 mysql 表设计中避免 bufferpool 频繁过期，隔离动静表
- 大到架构设计中的图片、静态资源等缓存加速。例如，CDN 加速。


本质上都体现的一样的思路，即**加速/缓存访问变换频次小的**。



**CDN 场景中，将静态资源和动态 API 分离，也是体现了隔离的思路**

- 降低应用服务器负载，静态文件访问负载全部通过CDN。
- 对象存储存储费用最低。
- 海量存储空间，无需考虑存储架构升级。
- 静态 CDN 带宽加速，延迟低。







**false sharing 简介**

CPU 缓存系统中是以缓存行（cache line）为单位存储的。目前主流的 CPU Cache 的 Cache Line 大小都是 64Bytes。

在多线程情况下，如果需要修改“共享同一个缓存行的变量”，就会无意中影响彼此的性能，这就是伪共享（False Sharing）。**当不同的线程同时读写同一 cache line上不同数据时就可能发生 false sharing。false sharing 会导致多核处理器上严重的系统性能下降。**

例如：一个结构体中有两个字段，两个字段大小在同一个 cacheline 上，而有两个 Goroutine 同时修改这个结构体中的不同字段的时候，会引发 伪共享（false sharing）。这就会影响性能。

解决方法：可以使用标准库，或者在结构体中的两个字段之间，加一些空的字段，使得两个字段分别在两个cacheline中。



**数据库表设计来避免 bufferpool 频繁过期**

![动静分离稿件表设计](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E5%8A%A8%E9%9D%99%E9%9A%94%E7%A6%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E8%AE%BE%E8%AE%A1.png?raw=true)


- archive: 稿件表
    - 存储稿件的名称、作者、分类、tag、状态等信息，表示稿件的基本信息。
    - 在一个投稿流程中，一旦稿件创建改动的频率比较低。
- archive_stat: 稿件统计表
    - 表示稿件的播放、点赞、收藏、投币数量，比较高频的更新。
    - 随着稿件获取流量，稿件被用户所消费，各类计数信息更新比较频繁。


MySQL BufferPool 是用于缓存 DataPage 的，DataPage 可以理解为缓存了表的行，那么如果频繁更新 DataPage 不断会置换，会导致命中率下降的问题，所以我们在表设计中，仍然可以沿用类似的思路，其主表基本更新，在上游 Cache 未命中，透穿到 MySQL，仍然有 BufferPool 的缓存。




## 轻重隔离


### 1. 核心隔离

业务按照 Level 进行资源池划分(L0/L1/L2)。越重要的服务隔离程度越高。核心业务独立部署，非核心业务共享资源
- 核心/非核心的故障域的差异隔离(机器资源、依赖资源)。
- 多集群，通过冗余资源来提升吞吐和容灾能力。



### 2. 快慢隔离

我们可以把服务的吞吐想象为一个池，当突然洪流进来时，池子需要一定时间才能排放完，这时候其他支流在池子里待的时间取决于前面的排放能力，耗时就会增高，对小请求产生影响。

**解决思路：可以根据流数据的 metadata 中的某些字段做分类，然后再路由到下游的 kafka 中继续执行**



日志传输体系的架构设计流程：
- 整个流都会投放到一个 kafka topic 中(早期设计目的: 更好的顺序IO)
- 流内会区分不同的 logid，logid 会有不同的 sink 端（目的）
- 它们之前会出现差速，比如 HDFS 抖动吞吐下降，ES 正常水位，全局数据就会整体反压。
- 按照各种纬度隔离：sink、部门、业务、logid、重要性(S/A/B/C)。业务日志也属于某个 logid，日志等级就可以作为隔离通道。





### 3. 热点隔离

何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行缓存。比如：
- 小表广播: 从 remotecache 提升为 localcache，app 定时更新，甚至可以让运营平台支持广播刷新 localcache。
- 主动预热: 比如直播房间页高在线情况下 bypass 监控主动防御。




## 物理隔离

### 1. 线程隔离


主要通过线程池进行隔离，也是实现服务隔离的基础。把业务进行分类并交给不同的线程池进行处理，当某个线程池处理一种业务请求发生问题时，不会讲故障扩散和影响到其他线程池，保证服务可用.


我们看一下单一线程池的模型：

![tomcat单一线程池](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E7%BA%BF%E7%A8%8B%E9%9A%94%E7%A6%BBtomcat%E5%8D%95%E4%B8%80%E7%BA%BF%E7%A8%8B%E6%B1%A0.png?raw=true)

存在问题：
- 用户请求到达后，从线程池中拿一个线程处理，这个请求有可能是不同服务的请求。
- 如果推荐服务突然停止了，那么大量用户请求会通过线程池到推荐服务，很快就会把线程池资源吃完，造成整个系统瘫痪。


通过线程隔离的思路来解决：

![tomcat多线程池模型](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E7%BA%BF%E7%A8%8B%E9%9A%94%E7%A6%BB%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%88%86%E7%A6%BB.png?raw=true)


思路：
- 如果推荐服务停止，只是推荐服务的这个线程池资源用完，停止服务，但对于其他服务的线程池不会影响
- 在推荐服务停止后，快速报错，这样很容易对该服务做降级处理。



**对于golang：**
- golang 中没有线程池，而是使用的 Goruotine
- 只要 保持 Goroutine 的总量不会超过某个数目，就可以保持这个线程池的作用不会被耗尽
- 保持 Goroutine 的数目，可以使用 超时控制等。
- 对于 Go 来说，所有 IO 都是 Nonblocking，且托管给了 Runtime，只会阻塞Goroutine，不阻塞 M，我们只需要考虑 Goroutine 总量的控制，不需要线程模型语言的线程隔离。



### 2. 进程隔离

进程：我们现在一般使用容器化服务，跑在 k8s 上这就是一种进程级别的隔离


### 3. 集群隔离

多套集群方案，即逻辑上是一个应用，物理上部署多套应用，通过 cluster 区分。

非常重要的服务我们可以部署多套，在物理上进行隔离，常见的有异地部署，也可能就部署在同一个区域



## 隔离的实例

- 早期转码集群被超大视频攻击，导致转码大量延迟。
- 入口Nginx(SLB)故障，影响全机房流量入口故障。
- 缩略图服务，被大图实时缩略吃完所有 CPU，导致正常的小图缩略被丢弃，大量503。
- 数据库实例 cgroup 未隔离，导致大 SQL 引起的集体故障。
- INFO 日志量过大，导致异常 ERROR 日志采集延迟。





# 超时控制


超时控制，本质是**组件能够快速失效(fail fast)**，因为不希望等到断开的实例直到超时。这不仅浪费资源，而且还会让用户体验变得更差。我们的服务是互相调用的，所以在这些延迟叠加前，应该特别注意防止那些超时的操作。

![网络超时](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E7%BD%91%E7%BB%9C%E8%B6%85%E6%97%B6.png?raw=true)

造成网络超时的原因有：
- 网路传递具有不确定性。
- 客户端和服务端不一致的超时策略导致资源浪费。例如客户端设置2s超时，服务端设置1s超时，这就会使得客户端白白等待。
- “默认值”策略。比如某些配置字段，设置默认值。

注意：  

​		**高延迟服务导致 client 浪费资源等待，使用超时传递: 进程间传递 + 跨进程传递。**

​		也就是客户端给服务端发送消息时，带上超时时间，防止两边不一致。



**超时控制是微服务可用性的第一道关，良好的超时策略，可以尽可能让服务不堆积请求，尽快清空高延迟的请求，释放 Goroutine。**



## 如何控制超时策略

实际业务开发中，我们依赖的微服务的超时策略并不清楚，或者随着业务迭代耗时超生了变化，意外的导致依赖者出现了超时。


在服务端，要义好潜在的超时配置 latency SLO，**更新到 gRPC Proto 定义中，服务后续迭代，都应保证 SLO。**

因为 无论是客户端还是服务端，都会查看 gRPC Proto 文件的内容。因此也是一种约定。


避免出现意外的默认超时策略，或者意外的配置超时策略。
- kit 基础库兜底默认超时，比如 100ms，进行配置防御保护，避免出现类似 60s 之类的超大超时策略。
- 配置中心公共模版，对于未配置的服务使用公共配置。




注意：
- **SLI**: Service Level Indicator 服务水平指示器
    - 对于业务来说是最重要的指标。比如，对于网站来说，一个常见的 SLI 是请求得到正常响应的百分比。 
- **SLO**: Service Level Object 服务水平目标
    - 是围绕 SLI 构建的目标。
    - 通常是一个百分比，并与一个时间范围挂钩。比如，月度、季度、年度等。
    - 通常用一连串 9 来度量。如果脱离了时间的度量，SLO 的意义就不大了。 
    - SLO 例如：
        - 90%（1个9的正常运行时间）：这意味着10%的停机时间，也就是说在过去的30天里停机了3天。
        - 99%（2个9的正常运行时间）：意味着在过去30天中有1%，或者说7.2小时的停机时间。
        - 99.9%（3个9的正常运行时间）：意味着0.1%，或者说43.2分钟的停机时间。
        - 99.95%（3.5个9的正常运行时间）：意味着0.05%，或者说21.6分钟的停机时间。
        - 99.99%（4个9的正常运行时间）：意味着0.01%，或者说4.32分钟的停机时间。
        - 99.999%（5个9的正常运行时间）：意味着0.001%，或者说26秒的停机时间。
- **SLA**: Service Level Agreement 服务水平协议
    - 是企业围绕 SLO 发布的协议。它要求在不满足SLO时向客户补偿的协议。



## 超时传递


**使用场景**: 当上游服务已经超时返回 504，但下游服务仍然在执行，会导致浪费资源做无用功。

**超时传递**：指的是把当前服务的剩余 Quota 传递到下游服务中，继承超时策略，控制请求级别的全局超时控制。

超时传递也就是说，每一个服务所花费的时间总和，要小于全局的超时时间。如下图所示：

![超时传递](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E8%B6%85%E5%B8%82%E6%8E%A7%E5%88%B6%E4%B8%AD%E7%9A%84%E8%B6%85%E6%97%B6%E4%BC%A0%E9%80%92.png?raw=true)


过程：
- 全局的超时时间为 1s 也就是 1000ms
- Service A 调用 redis 的时间为 100ms，那么剩下的超时时间就为 900ms
- Service A 调用 Service B 的时间为 500ms，那么剩下的超时时间为 900ms-500ms=400ms
- Service A 最后调用 数据库服务时，数据库有一个超时时间，还有一个剩下的全局超时时间，那么就要选择一个最小值作为调用数据库服务的超时时间，min（500，400）= 400，数据库服务花费 350ms，小于 400ms，因此不会超时



总结：
- 超时传递就是每个阶段的花费都要计算在内
- 在调用每个服务时，服务自己的超时时间，和全局剩余的超时时间，取最小值，作为该操作阶段的超时时间



**进程内超时控制**

一个请求在每个阶段(网络请求)开始前，就要检查是否还有足够的剩余时间来处理请求，以及继承他的超时策略，使用 Go 标准库的 context.WithTimeout。



**gRPC 天然支持超时控制**

![gRPC超时传递](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/gRPC%E8%B6%85%E6%97%B6%E4%BC%A0%E9%80%92.png?raw=true)

过程：
- A gRPC 请求 B，1s超时。
- B 使用了300ms 处理请求，再转发请求 C。
- C 配置了600ms 超时，但是实际只用了500ms。
- 到其他的下游，发现余量不足，取消传递。

原理：
- 在需要强制执行时，下游的服务可以覆盖上游的超时传递和配额。
- 在 gRPC 框架中，会依赖 gRPC Metadata Exchange，基于 HTTP2 的 Headers 传递 grpc-timeout 字段，自动传递到下游，构建带 timeout 的 context。



## 超时实例

- SLB 入口 Nginx 没配置超时导致连锁故障。
- 服务依赖的 DB 连接池漏配超时，导致请求阻塞，最终服务集体 OOM。
- 下游服务发版耗时增加，而上游服务配置超时过短，导致上游请求失败。





# 过载保护

这一节主要针对的单机的限流，也就是在微服务内部使用某种方法，或者使用某种标准库来实现的限流。



## 1. 令牌桶算法


### 令牌桶算法介绍

令牌桶算法，其实就是：一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌（令牌也就是数据包）。
- 令牌桶就是想象有一个固定大小的桶，系统会以恒定速率向桶中放 Token，桶满则暂时不放。
- 用户则从桶中取 Token，如果有剩余 Token 就可以一直取。如果没有剩余 Token，则需要等到系统中被放置了 Token 才行。


令牌桶的算法描述如下：

![令牌桶](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E9%99%90%E6%B5%81%E4%B9%8B%E4%BB%A4%E7%89%8C%E6%A1%B6%E7%AE%97%E6%B3%95.png?raw=true)


**添加令牌：**
- 假设限制 10rps/s，则按照 100 毫秒的固定速率往桶中添加令牌。（1s添加10个token，也就是1s处理10个请求）
- 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。
- **当一个 n 个字节大小的数据包到达，将从桶中删除 n 个令牌，接着数据包被发送到网络上。**
如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。

注意：**一个令牌相当于1个字节大小，当一个请求有n字节大小，就需要有n个令牌**

**从桶中消费令牌：**
- 用户从桶中获取请求，并且处理，允许消费请求的速率可以突发，也就是突然很大，只有向桶中添加令牌时限速。
- 如果桶中没有令牌，则拒绝消费


**两种特殊情况：**
- 如果桶的容量为 0，那么相当于禁止请求，因为所有的令牌都被丢弃了
- 如果令牌放置速率为无穷大，那么相当于没有限制



令牌桶最常见的时标准库 **golang.org/x/time/rate**


## `golang.org/x/time/rate` 标准库的使用


### 方法索引

#### 限流器的速率
```go
type Limit
    func Every(interval time.Duration) Limit
```
- `type Limit` : 表示令牌桶的最大速率
- `Every()` : 将时间间隔转化为 Limit 令牌桶的速率，也就是每间隔一段时间，处理一个请求




#### 限流器的相关方法
```go
type Limiter
    func NewLimiter(r Limit, b int) *Limiter
    func (lim *Limiter) Allow() bool
    func (lim *Limiter) AllowN(now time.Time, n int) bool
    func (lim *Limiter) Burst() int
    func (lim *Limiter) Limit() Limit
    func (lim *Limiter) Reserve() *Reservation
    func (lim *Limiter) ReserveN(now time.Time, n int) *Reservation
    func (lim *Limiter) SetBurst(newBurst int)
    func (lim *Limiter) SetBurstAt(now time.Time, newBurst int)
    func (lim *Limiter) SetLimit(newLimit Limit)
    func (lim *Limiter) SetLimitAt(now time.Time, newLimit Limit)
    func (lim *Limiter) Wait(ctx context.Context) (err error)
    func (lim *Limiter) WaitN(ctx context.Context, n int) (err error)
```
- `type Limiter` : 限流器, 实现了一个令牌桶
- `NewLimiter(r Limit, b int) *Limiter` : 返回一个限流器，该限制器允许事件的发生率达到 r，并允许突发最多 b 个令牌。
- `Allow() bool` : 相当于 `AllowN(time.Now(), 1)`
- `AllowN(now time.Time, n int)` : 判断在某个时间，限流器中是否还有至少 n 个 Token，如果有，返回 true，否则返回 False
- `Burst() int` : 返回突发量大小，突发量表示一次允许消耗的最大令牌数，也就是令牌桶的大小
- `Limit()` : 返回允许的最大速率
- `Reserve()` : 相当于 `ReserveN(time.Now(), 1)`
- `ReserveN(now time.Time, n int) *Reservation` : 输入预约到的时间now, 预约的数量n，判断到now时间点是否有n个token，返回一个或多个未来token的预订以及调用者在使用前必须等待的时长。
- `SetBurst(newBurst int) *Reservation` : 相当于 `SetBurstAt(time.Now(), newBurst)`
- `SetBurstAt(now time.Time, newBurst int)` : 为限流器设置一个新的 突发量，设置桶大小
- `SetLimit(newLimit Limit)` : 相当于 `SetLimitAt(time.Now(), newLimit)`
- `SetLimitAt(now time.Time, newLimit Limit)` : 为限流器设置一个新的 速率。注意：如果设置了新的速率和新的突发量，之前的 Reservation 就可能不能充分利用
- `Wait(ctx context.Context)` : 相当于 `WaitN(ctx, 1)`
- `WaitN(ctx context.Context, n int)` : WaitN(ctx, n) 表示如果存在 n 个令牌就直接转发，不存在我们就等，等待存在为止，传入的 ctx 的 Deadline 就是等待的 Deadline






#### 预约令牌的相关方法


```go
type Reservation
func (r *Reservation) Cancel()
func (r *Reservation) CancelAt(now time.Time)
func (r *Reservation) Delay() time.Duration
func (r *Reservation) DelayFrom(now time.Time) time.Duration
func (r *Reservation) OK() bool
```
- `type Reservation` : Reservation可以理解成预定令牌的操作，timeToAct是本次预约需要等待到的指定时间点才有足够预约的令牌。
- `Cancel()` : 相当于 `CancelAt(time.Now())`
- `CancelAt(now time.Time)` : CancelAt 用于取消预约令牌操作，如果有需要还原的令牌，则将需要还原的令牌重新放入到令牌桶中。
- `Delay() time.Duration` : 相当于 `DelayFrom(time.Now())`
- `DelayFrom(now time.Time) time.Duration` : DelayFrom返回Reservation在执行保留操作之前必须等待的持续时间。持续时间为零意味着立即采取行动。 InfDuration表示限制器无法在最大等待时间内授予此保留中请求的令牌。
- `OK() bool` : 确定返回限流器是否可以在最大等待时间内提供请求数量的令牌。如果OK为false，则Delay返回InfDuration，而Cancel不执行任何操作。





### 使用实例

在 web 框架 GIn 中，添加了一个限流的中间件

```go
package main

import (
	"context"
	"github.com/gin-gonic/gin"
	"golang.org/x/time/rate"
	"net/http"
	"sync"
	"time"
)

//  传入参数：
//  r : rate.Limit 桶的速率
//  b : 桶的容量
//  t : 超时时间
func MyNewLimiter(r rate.Limit, b int, t time.Duration) gin.HandlerFunc {
	//  针对每一个 用户 ip 进行限流
	limiters := &sync.Map{}

	return func(ctx *gin.Context) {
		//  针对每一个 ip 对应一个限流器
		key := ctx.ClientIP()
		l, _ := limiters.LoadOrStore(key, rate.NewLimiter(r, b))

		//  设置超时时间，通过 gin 的 ctx 生成一个 带有超时时间的 子ctx
		timeoutCtx, cancel := context.WithTimeout(ctx, t)
		defer cancel()

		if err := l.(*rate.Limiter).Wait(timeoutCtx); err != nil {
			//  如果超时，直接返回 429
			ctx.AbortWithStatusJSON(http.StatusTooManyRequests, gin.H{"error": err})
		}
		ctx.Next()
	}
}

func main() {
	e := gin.Default()
	// 新建一个限速器，允许突发 10 个并发，限速 3rps，超过 500ms 就不再等待
	e.Use(MyNewLimiter(3, 10, 500*time.Millisecond))
	e.GET("ping", func(c *gin.Context) {
		c.String(http.StatusOK, "pong")
	})
	e.Run(":1234")
}
```


压力测试：使用 go-stress-testing-win 工具来进行压力测试，测试结果为：
- -c 表示并发数，一共并发20个请求
- -n 表示单个并发的请求数
```go
C:\Users\84212>go-stress-testing-win -c 20 -n 1 -u http://127.0.0.1:1234/ping

 开始启动  并发数:20 请求数:1 请求参数:
request:
 form:http
 url:http://127.0.0.1:1234/ping
 method:GET
 headers:map[]
 data:
 verify:statusCode
 timeout:30s
 debug:false



─────┬───────┬───────┬───────┬────────┬────────┬────────┬────────┬────────┬────────┬────────
 耗时│ 并发数│ 成功数│ 失败数│   qps  │最长耗时│最短耗时│平均耗时│下载字节│字节每秒│ 错误码
─────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┼────────
   0s│     20│     11│      9│  149.32│  348.03│   55.04│  133.94│     152│     420│200:11;429:9


*************************  结果 stat  ****************************
处理协程数量: 20
请求总数（并发数*请求数 -c * -n）: 20 总请求时间: 0.361 秒 successNum: 11 failureNum: 9
*************************  结果 end   ****************************
```

可以观察结果：
- 请求一共成功了 11 个，我们限流器的令牌桶大小设置为 10， 速率设置为 3，允许突发请求量也就是 10，超过500ms的等待就算超时
- 因此第十一个请求一共花了 348 ms，因此没有超时，也成功了，其他请求都超过 500 ms 超时丢弃。



### 源码分析


#### 1. Limit 类型及方法


- Limit 类型其实就是 float64 类型，表示的是令牌桶的速率，也就是 一秒向桶中放多少个令牌
```go
type Limit float64
```
- Limit 类型的 Every 方法，将向桶中放入令牌间隔时间，转化成 Limit 类型，也就是 rps/s
```go
func Every(interval time.Duration) Limit {
    # 如果间隔时间小于0，那么返回最大速率 Inf = Limit(math.MaxFloat64)
	if interval <= 0 {
		return Inf
	}
	# Token 放入桶的间隔时间，转成 每秒放入多少个Token，也就是 n s/个 -> 1/n 个/s
	return 1 / Limit(interval.Seconds())
}
```



#### 2. Limiter 和 Reservation 结构体

- Limiter 结构体，就是限流器的实现
```go
type Limiter struct {
	mu     sync.Mutex           //  互斥锁
	limit  Limit                //  令牌桶的 速率
	burst  int                  //  令牌桶的 容量大小
	tokens float64              //  令牌桶中可用的 令牌数量
	last time.Time              //  上次更新 token 的时间
	lastEvent time.Time         //  记录速率受限制(桶中没有令牌)的时间点，该时间点可能是过去的，也可能是将来的(Reservation预定的结束时间点)
}
```
- Reservation 结构体，我的理解这个结构体的意思是，用户可以预约消费令牌，也就是说预约在多久后，消费多少个了你挂牌。
    - Reservation 可以理解成预定令牌的操作，timeToAct 是本次预约需要等待到的指定时间点才有足够预约的令牌。
```go
type Reservation struct {
	ok        bool              //  到截止时间，是否可以获取到令牌
	lim       *Limiter          //  限流器指针
	tokens    int               //  预约获取的令牌数量
	timeToAct time.Time         //  需要等待到 timeToAct 的时间点
	limit Limit                 //  令牌桶的速率，由于令牌桶的速率是可以修改，因此这里需要保存
}
```


**Reservation 预约的原理：**
- 现创建一个 限流器，每秒一个token，容量大小为10
- 当前时刻，令牌桶中一共有 2 个可以消费的 Token，而此时用户希望预约 7 个Token
    - 此时，令牌桶中的Token数量不足以预定，还需要再进来5个Token，也就是再过 5s 的时间
    - 那么 timeToAct 的时间点就是 5s 后的时间点
    - 而此时令牌桶中，更新 Token 数量为 -5 


#### 3. Limiter 结构体相关方法


##### 消费桶中 token 的方法

**创建 Limiter 限流器**
- `NewLimiter` : 指定令牌桶的速率和容量，返回一个新创建的 Limiter 结构体，也就是限流器
```go
func NewLimiter(r Limit, b int) *Limiter {
	return &Limiter{
		limit: r,
		burst: b,
	}
}
```

**Allow、Reserve、Wait 方法**
Limiter 有三个核心方法，这三个方法每调用一次都会消耗一个令牌，这三个方法的区别在于没有令牌时，他们的处理方式不同

- `Allow`： 如果没有令牌，则直接返回false
    - Allow 方法，其实就是调用的 AllowN
    - AllowN 方法，最终就是调用的 reserveN
```go
func (lim *Limiter) Allow() bool {
	return lim.AllowN(time.Now(), 1)
}

func (lim *Limiter) AllowN(now time.Time, n int) bool {
	return lim.reserveN(now, n, 0).ok
}
```
- Reserve：如果没有令牌，则返回一个reservation
    - Reserve 方法，其实就是调用的 ReserveN
    - ReserveN 方法，最终就是调用的 reserveN
```go
func (lim *Limiter) Reserve() *Reservation {
	return lim.ReserveN(time.Now(), 1)
}

func (lim *Limiter) ReserveN(now time.Time, n int) *Reservation {
	r := lim.reserveN(now, n, InfDuration)
	return &r
}
```
- Wait：如果没有令牌，则等待直到获取一个令牌或者其上下文被取消
    - Wait 方法，其实就是调用的 WaitN
    - WaitN 方法，最终就是调用的 reserveN
```go
func (lim *Limiter) Wait(ctx context.Context) (err error) {
	return lim.WaitN(ctx, 1)
}

func (lim *Limiter) WaitN(ctx context.Context, n int) (err error) {
    //  拿到 令牌桶中的速率和容量，由于 其他 Goroutine 有可能修改 ，因此需要加锁
	lim.mu.Lock()
	burst := lim.burst
	limit := lim.limit
	lim.mu.Unlock()

    //  如果 需要获取的 token 数量 大于 桶的 容量，报错 返回 err
	if n > burst && limit != Inf {
		return fmt.Errorf("rate: Wait(n=%d) exceeds limiter's burst %d", n, burst)
	}
	// 如果 传入 context 已经 cancel 掉了，那就直接返回
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}
	
	// 计算 需要等待的时间，InfDuration 表示需要等待无穷大的时间
	now := time.Now()
	waitLimit := InfDuration
	
	# 从 ctx 中取出 timeout 时间，并且获取到 还剩多久时间会 timeout
	if deadline, ok := ctx.Deadline(); ok {
		waitLimit = deadline.Sub(now)
	}
	// 调用 reservN 方法，预约 令牌桶中的 令牌，并且传入 超时时间
	r := lim.reserveN(now, n, waitLimit)
	//  如果无法预约，直接返回错误
	if !r.ok {
		return fmt.Errorf("rate: Wait(n=%d) would exceed context deadline", n)
	}
	
	//  如果 可以预约到，计算 还需要等待多长时间
	//  DelayFrom 函数是 Reservation 结构体的方法
	delay := r.DelayFrom(now)
	
	//  如果不需要等待，直接返回
	if delay == 0 {
		return nil
	}
	
	//  如果需要等待，则设置一个定时器
	t := time.NewTimer(delay)
	defer t.Stop()
	
	//  有两种情况：
	//  1. 如果 需要等待时间 的定时器到了，那么就可以直接返回，表示在 超时时间内 获取到了 token
 	//  2. 如果 超时时间 到了，那么就 取消 Reservation ，并且返回 timeout err
	select {
	case <-t.C:
		// We can proceed.
		return nil
	case <-ctx.Done():
		// Context was canceled before we could proceed.  Cancel the
		// reservation, which may permit other events to proceed sooner.
		r.Cancel()
		return ctx.Err()
	}
}
```



**最关键的内置函数  reserveN**

- 内置函数 `reserveN`: 
    - 传入参数：
        - now ： 需要消费 token 的时间点
        - n : 需要消费多少个 token
        - maxFutureReserve: 能够等待的最长时间，也就是超时时间
```go
func (lim *Limiter) reserveN(now time.Time, n int, maxFutureReserve time.Duration) Reservation {
	//  由于 限流器 Limiter 可以修改速率和容量，防止其他Goroutine修改，因此需要加锁
	lim.mu.Lock()

    //  如果 限流器 没有限速，就直接返回，要多少返回多少
	if lim.limit == Inf {
		lim.mu.Unlock()
		return Reservation{
			ok:        true,
			lim:       lim,
			tokens:    n,
			timeToAct: now,
		}
	}
    
    // advance 方法会去计算last-now有多少个 token
    // 后面会讲到，now 其实就是传入的时间，但是 last 可能会变
	now, last, tokens := lim.advance(now)

	// n 表示想预约的token数量，
	//  tokens 表示 last-now 的token数 减去 想预约的token数 还剩下的token数
	//  如果 token 为负值，表示 到 now 时间点，令牌桶中还没有 n 个令牌供消费
	tokens -= float64(n)

	// 计算 需要等待的时间
	var waitDuration time.Duration
	if tokens < 0 {
		waitDuration = lim.limit.durationFromTokens(-tokens)
	}

	// 如果需要消费的token数量 比 桶的容量大，则false
	// 如果 需要等待的时间 比 超时时间还长，也是 false
	ok := n <= lim.burst && waitDuration <= maxFutureReserve

	// 创建 Reservation 对象
	r := Reservation{
		ok:    ok,
		lim:   lim,
		limit: lim.limit,
	}
	
	//  如果满足消费，那么就 把 token 数 和 timeToAct 时间 放入 Reservation 对象
	if ok {
		r.tokens = n
		r.timeToAct = now.Add(waitDuration)
	}

	// 并且成功后，需要更新 限流器 Limiter 对象中的 last tokens数目，和 lastEvent。
	if ok {
		lim.last = now
		lim.tokens = tokens
		lim.lastEvent = r.timeToAct
	} else {
		lim.last = last
	}

	lim.mu.Unlock()
	return r
}
```
- `reserveN` 函数中使用到的 `advance` 函数，返回到 now 时间的token数量
    - 传入参数：
        - now : 当前时间
    - 返回：
        - newNow : 上次更新 token 的时间
        - newLast : 上次限速的时间
        - newToken : token 数量
```go
func (lim *Limiter) advance(now time.Time) (newNow time.Time, newLast time.Time, newTokens float64) {
    //  获取 限流器 Limiter 中 last，也就是 上次更新 token 的时间
    //  注意：传入 的 now 不能比 last 早，否则计算 maxElapsed 是错的
	last := lim.last
	if now.Before(last) {
		last = now
	}

	// 这里为了防止溢出，先计算了将桶填满需要花费的最大时间
	//  float64(lim.burst) - lim.tokens 表示 填满桶还需要多好 Token
	//  durationFromTokens 函数 根据 tokens 的数量计算需要花费的时间
	maxElapsed := lim.limit.durationFromTokens(float64(lim.burst) - lim.tokens)
	
	//  最近更新时间 到 传入的 now 时间，计算 这一时间段
	elapsed := now.Sub(last)
	//  如果 需要计算的时间 大于 填满桶的最大时间的话，就取最大值
	if elapsed > maxElapsed {
		elapsed = maxElapsed
	}

	// 计算这段时间生成的 token 数量，如果大于桶的容量，就取桶的容量
	//  根据需要计算的 last - now 时间段，计算出一共可以添加多少 Token
	delta := lim.limit.tokensFromDuration(elapsed)
	//  tokens 表示 总的 token 数
	tokens := lim.tokens + delta
	//  如果总数 超过了 令牌桶的 容量，则返回最大值，也就是桶的容量
	if burst := float64(lim.burst); tokens > burst {
		tokens = burst
	}

	return now, last, tokens
}
```

**durationFromTokens 函数**

- `durationFromTokens` 函数就是 根据 tokens 的数量计算需要花费的时间
```go
func (limit Limit) durationFromTokens(tokens float64) time.Duration {
    //  token 数量(单位个) / 速率(个/s) = 添加这些Token需要花费的时间(单位 s)
	seconds := tokens / float64(limit)
	//  把 秒 转成 纳秒 返回
	return time.Nanosecond * time.Duration(1e9*seconds)
}
```


**tokensFromDuration 函数**
- `tokensFromDuration` 函数就是 根据时间计算 tokens 的数量
```go
func (limit Limit) tokensFromDuration(d time.Duration) float64 {
    //  时间段(单位s) * 速率(个/s) = 这段时间一共可以添加多少Token(单位个)
    //  这里通过拆分整数和小数部分可以减少时间上的误差
    
    //  整数部分 
 	sec := float64(d/time.Second) * float64(limit)
	//  小数部分 
	nsec := float64(d%time.Second) * float64(limit)
	
	//  秒 转成 纳秒 返回
	return sec + nsec/1e9
}
```




##### 动态调整令牌桶


- `Limit()` : 返回速率
- `Burst()` : 返回令牌桶的容量
    - 注意：读取数据时，加锁
```go
func (lim *Limiter) Limit() Limit {
	lim.mu.Lock()
	defer lim.mu.Unlock()
	return lim.limit
}

func (lim *Limiter) Burst() int {
	lim.mu.Lock()
	defer lim.mu.Unlock()
	return lim.burst
}
```
- `SetBurst`、`SetBurstAt` : 设置 令牌桶的容量
```go
func (lim *Limiter) SetBurst(newBurst int) {
	lim.SetBurstAt(time.Now(), newBurst)
}

func (lim *Limiter) SetBurstAt(now time.Time, newBurst int) {
	lim.mu.Lock()
	defer lim.mu.Unlock()

    //  计算 last-now 一共可以添加多少个 token
	now, _, tokens := lim.advance(now)

    //  更新
	lim.last = now
	lim.tokens = tokens
	lim.burst = newBurst
}
```
- `SetLimit`、`SetLimitAt` : 设置 令牌桶的容量
    - 逻辑同上



#### 3. Reservation 结构体的相关方法


- `OK()` : 很简单，返回 Reservation 结构体中 ok 属性
```go
func (r *Reservation) OK() bool {
	return r.ok
}
```
- `Delay`、`DelayFrom` : 返回还需要等待多久才可以被执行
```go
func (r *Reservation) Delay() time.Duration {
	return r.DelayFrom(time.Now())
}

func (r *Reservation) DelayFrom(now time.Time) time.Duration {
	if !r.ok {
		return InfDuration
	}
	//  求出当前事件 与 真正执行的事件的 间隔返回
	delay := r.timeToAct.Sub(now)
	if delay < 0 {
		return 0
	}
	return delay
}
```
- `Cancel`、`CancelAt` : 取消 Reservation 的预定
```go
func (r *Reservation) Cancel() {
	r.CancelAt(time.Now())
	return
}

func (r *Reservation) CancelAt(now time.Time) {
    //  如果 Reservation 对象的 ok 是属性是 false，也就是没有预约成功 直接返回
	if !r.ok {
		return
	}

    
	r.lim.mu.Lock()
	defer r.lim.mu.Unlock()

    // 如果 令牌桶的速率没有限制，或者 预约的消费token数为0，或者 已经消费掉了，就直接返回
	if r.lim.limit == Inf || r.tokens == 0 || r.timeToAct.Before(now) {
		return
	}

    //  r.lim.lastEvent 表示 某个到达限速的时间点，有可能是本次 Reservation 的结束时间，也有可能是后面某个 Reservation 的结束时间
    //  减数 为 计算 限速时间点 到 本次Reservation 结束的时间点 之间产生的 tokens 数目
    //  被减数是 本次 Reservation 需要消费的总 tokens 数
    //  减的结果就是 需要还原的 token 数目
    //  1. 这个token数目是正数，表示 本次 Reservation 限速，lastEvent 就是本次 Reservation 的时间，那么 restoreTokens 就是 总的消费数 - 开始限速到消费Token的间隔 ，就是 计算需要还的 token 数量
    //  2. 这个token数目是负数，表示 全都预消费完了
	restoreTokens := float64(r.tokens) - r.limit.tokensFromDuration(r.lim.lastEvent.Sub(r.timeToAct))
	if restoreTokens <= 0 {
		return
	}
	//  从 cancel 的这个时间点，计算令牌桶中的 token 数目
	now, _, tokens := r.lim.advance(now)
	
	// 将 token 数目 加上 之前 还原的 token 数目，
	tokens += restoreTokens
	
	//  如果桶 的容量 小于 token 总数，则将 token 数设置为容量
	if burst := float64(r.lim.burst); tokens > burst {
		tokens = burst
	}
	// 更新限流器
	r.lim.last = now
	r.lim.tokens = tokens
	
	// 如果相等说明后面没有新的 token 消费，所以将状态重置到上一次, 也就是 cancel 的这个 Reservation 的上一次 Reservation 
	if r.timeToAct == r.lim.lastEvent {
		prevEvent := r.timeToAct.Add(r.limit.durationFromTokens(float64(-r.tokens)))
		if !prevEvent.Before(now) {
			r.lim.lastEvent = prevEvent
		}
	}

	return
}
```

注意： 对 `	restoreTokens := float64(r.tokens) - r.limit.tokensFromDuration(r.lim.lastEvent.Sub(r.timeToAct))` 的解读：

第一种情况，cancel 的 Reservation 后有事件预定。
- 事件1，从第1s开始预约，假设 timeToAct 也就是执行时间为 第10s，这时，各个属性为：
    - last = 1s
    - lastEvent = 1s
    - timeToAct = 10s
- 事件2，从第3s开始预约，假设 timeToAct 也就是事件2的执行事件为 第18s，这时，各个属性为：
    - last = 3s
    - lastEvent = 3s
    - timeToAct = 18s
- 在 第 5s 时，事件1 cancel，这时，restoreTokens 计算为：
    - 其实，cancel的 Reservation 的 timeToAct ~ lastEvent 的时间段，也就是 3 ~ 10s ，那么能保留的就是 1-3s 的token。
- 为什么这里要减去一个 cancel的 Reservation 的 timeToAct ~ lastEvent 的时间段 tokens 数目，原因就是为了减小对后面新预定的事件的影响。原因：**在第3s的适合事件2已经开始预约token了，因此3s后产生的token已经有为事件2的做准备了，也就是已经关联到事件2了。**
- 这样做就是为了不影响事件2，而让新来预定的事件的影响尽量小。



第二种情况，cancel 的 Reservation 后没有事件预定、
- 这种情况，cancel 的 Reservation 中的所有 Token 都可以还原。




## 2. 漏桶算法

### 漏桶算法介绍

**漏桶算法**：把请求比作是水，水来了都先放进桶里，并以限定的速度出水，当水来得过猛而出水不够快时就会导致水直接溢出，即拒绝服务。

![漏桶算法](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E9%99%90%E6%B5%81%E4%B9%8B%E6%BC%8F%E6%A1%B6%E7%AE%97%E6%B3%95.png?raw=true)


在漏斗中没有水的时候：
- 如果进水速率小于等于最大出水速率，那么，出水速率等于进水速率，此时，不会积水
- 如果进水速率大于最大出水速率，那么，桶以最大速率出水，此时，多余的水会积在桶中


在漏斗中有水的时候：
- 出水口以最大速率出水
- 如果桶未满，且有进水的话，那么这些水会积在桶中
- 如果桶已满，且有进水的话，那么这些水会溢出到桶之外




### `go.uber.org/ratelimit` 实现的 ratelimit


### 方法索引

#### 1. 相关的结构体：

```go
type Clock
type Limiter
type Option
```

- `type Clock` : Clock 是用一个时钟或模拟时钟实例化速率限制器所需的最小接口
- `type Limiter` : 限流器 用于限制某些进程，可能跨 goroutine。该流程希望在每次迭代之前调用 Take()，这可能会阻塞以抑制 goroutine。
- `type Option` : 空接口，表示传入的选项


#### 2. 方法

```go
func New(rate int, opts ...Option) Limiter
func NewUnlimited() Limiter
func Per(per time.Duration) Option
func WithClock(clock Clock) Option
func WithSlack(slack int) Option
```

- `New(rate int, opts ...Option) Limiter` : 创建一个 限流器，并且设置 出口速率，rate 表示每秒多少个请求进来
- `NewUnlimited() Limiter` : 创建一个 限流器，不限入口速率
- `Per(per time.Duration) Option` : 可以修改时间单位，例如：
    - New(100) 表示创建入口速率是 100个请求/秒
    - New(2, Per(60*time.Second)) 表示创建的入口速率是 2 个请求/分钟



限流器只有一个 Token 方法，执行这个方法的时候如果触发了 rps 限制则会阻塞住



### 使用示例：

**Take方法应该阻塞已确保满足 RPS**

```go
import (
	"fmt"
	"time"

	"go.uber.org/ratelimit"
)

func main() {
    rl := ratelimit.New(100) // per second

    prev := time.Now()
    for i := 0; i < 10; i++ {
        now := rl.Take()
        fmt.Println(i, now.Sub(prev))
        prev = now
    }

}

```



## 使用阈值作为限流策略的缺点

漏斗桶/令牌桶确实能够保护系统不被拖垮, 但不管漏斗桶还是令牌桶, 其防护思路都是设定一个指标：
- 当超过该指标后就阻止或减少流量的继续进入
- 当系统负载降低到某一水平后则恢复流量的进入。
- 但其通常都是被动的，其实际效果取决于限流阈值设置是否合理，但往往设置合理不是一件容易的事情。


这些其实都是采用漏斗桶/令牌桶的缺点, 总体来说就是太被动, 不能快速适应流量变化。因此我们需要一种自适应的限流算法，即: 过载保护，根据系统当前的负载自动丢弃流量。




## 过载保护(自适应限流)

**过载保护(自适应限流)：**  

计算系统临近过载时的峰值吞吐作为限流的阈值来进行流量控制，达到系统保护。



常用到的方法有：
- 服务器临近过载时，主动抛弃一定量的负载，目标是自保。
- 在系统稳定的前提下，保持系统的吞吐量。常见做法：利特尔法则, TCP bbr 算法。
- CPU、内存作为信号量进行节流。
- 队列管理: 队列长度、LIFO。
- 可控延迟算法: CoDel。





### 利特尔法则


![利特尔法则示意图](https://github.com/Nevermore12321/LeetCode/blob/blog/go%E8%BF%9B%E9%98%B6%E8%AE%AD%E7%BB%83%E8%90%A5/%E5%88%A9%E7%89%B9%E5%B0%94%E6%B3%95%E5%88%99%E6%BC%94%E7%A4%BA%E5%9B%BE.png?raw=true)


- 平均每分钟进店 2 个客人(λ)，每位客人从等待到完成交易需要 4 分钟(W)，那我们店里能承载的客人数量就是 2 * 4 = 8 个人
- 可以将 λ 当做 QPS， W 呢是每个请求需要花费的时间，那我们的系统的吞吐就是 L = λ * W ，所以我们可以使用利特尔法则来计算系统的吞吐量。




### 利特尔法则的启发思路

**当一台电脑的 cpu 达到 90% 时，获取过去 5s 内的最大的吞吐量，也就是说这就是自动计算出来的阈值，之后的请求流量如果不超过这个阈值，那就安全，超过了就可以丢弃。**



如何计算接近峰值时的系统吞吐？
- CPU: **使用一个独立的线程采样，每隔 250ms 触发一次。在计算均值时，使用了简单滑动平均去除峰值的影响。也就是滑动窗口内的cpu平均，平且设置一个 权值，例如过去 5s 的CPU使用情况乘以权值0.3，当前CPU的使用情况乘以权值0.7，这样就可以去除掉因为CPU抖动形成的毛刺峰值。**
- Inflight: 当前服务中正在进行的请求的数量。
- Pass&RT: 最近5s，pass 为每100ms采样窗口内成功请求的数量，rt 为单个采样窗口中平均响应时间



**kratos 自适应限流分析**
```go
(cpu > 800 OR (Now - PrevDrop) < 1s) AND (MaxPass * MinRt * windows / 1000) < InFlight
```

- cpu > 800 表示 CPU 负载大于 80% 进入限流
- (Now - PrevDrop) < 1s 这个表示只要触发过 1 次限流，那么 1s 内都会去做限流的判定，这是为了避免反复出现限流恢复导致请求时间和系统负载产生大量毛刺
- (MaxPass * MinRt * windows / 1000) < InFlight 判断当前负载是否大于最大负载
- InFlight 表示当前系统中有多少请求
- (MaxPass * MinRt * windows / 1000) 表示过去一段时间的最大负载 
    - MaxPass 表示最近 5s 内，单个采样窗口中最大的请求数
    - MinRt 表示最近 5s 内，单个采样窗口中最小的响应时间
    - windows 表示一秒内采样窗口的数量，默认配置中是 5s 50 个采样，那么 windows 的值为 10